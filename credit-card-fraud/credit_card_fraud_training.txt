 autotrain llm --train --project_name peft-tune-llama-credit-card-fraud --model meta-llama/Llama-2-7b-chat-hf --data_path . --use_peft --use_int4 --learning_rate 2e-4 --train_batch_size 6 --num_train_epochs 10 --trainer sft
⚠️ WARNING | 2023-11-21 22:49:45 | autotrain.cli.run_dreambooth:<module>:14 - ❌ Some DreamBooth components are missing! Please run `autotrain setup` to install it. Ignore this warning if you are not using DreamBooth or running `autotrain setup` already.
> INFO    Running LLM
> INFO    Params: Namespace(version=False, train=True, deploy=False, inference=False, data_path='.', train_split='train', valid_split=None, text_column='text', rejected_text_column='rejected', prompt_text_column='prompt', model='meta-llama/Llama-2-7b-chat-hf', model_ref=None, learning_rate=0.0002, num_train_epochs=10, train_batch_size=6, warmup_ratio=0.1, gradient_accumulation_steps=1, optimizer='adamw_torch', scheduler='linear', weight_decay=0.0, max_grad_norm=1.0, seed=42, add_eos_token=False, block_size=-1, use_peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, logging_steps=-1, project_name='peft-tune-llama-credit-card-fraud', evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=False, fp16=False, push_to_hub=False, use_int8=False, model_max_length=1024, repo_id=None, use_int4=True, trainer='sft', target_modules=None, merge_adapter=False, token=None, backend='default', username=None, use_flash_attention_2=False, log='none', disable_gradient_checkpointing=False, dpo_beta=0.1, func=<function run_llm_command_factory at 0x7f33f3b60dc0>)
> INFO    loading dataset from csv
Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 39.68it/s]
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:46<00:00, 53.27s/it]
> INFO    Using block size 1024
> INFO    creating trainer
  0%|                                                                                                                                       | 0/3340 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/ubuntu/llm/meta-llama/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.7061, 'learning_rate': 3.9520958083832336e-05, 'epoch': 0.2}                                                                                              
  3%|███                                                                                                                         | 84/3340 [07:51<4:49:50,  5.34s/it]/home/ubuntu/llm/meta-llama/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.1512, 'learning_rate': 7.904191616766467e-05, 'epoch': 1.14}                                                                                              
  5%|██████▏                                                                                                                    | 168/3340 [15:42<4:42:24,  5.34s/it]/home/ubuntu/llm/meta-llama/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.0177, 'learning_rate': 0.00011856287425149701, 'epoch': 2.09}                                                                                             
  8%|█████████▎                                                                                                                 | 252/3340 [23:33<4:34:45,  5.34s/it]/home/ubuntu/llm/meta-llama/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.009, 'learning_rate': 0.00015808383233532935, 'epoch': 3.04}                                                                                              
{'loss': 0.0062, 'learning_rate': 0.0001976047904191617, 'epoch': 3.23}                                                                                              
 10%|████████████▎                                                                                                              | 336/3340 [31:25<4:27:04,  5.33s/it]/home/ubuntu/llm/meta-llama/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.0055, 'learning_rate': 0.00019587491683300068, 'epoch': 4.18}                                                                                             
 13%|███████████████▍                                                                                                           | 420/3340 [39:16<4:19:33,  5.33s/it]/home/ubuntu/llm/meta-llama/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.0047, 'learning_rate': 0.0001914836992681304, 'epoch': 5.13}                                                                                              
 15%|██████████████████▌                                                                                                        | 504/3340 [47:07<4:12:15,  5.34s/it]/home/ubuntu/llm/meta-llama/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.0048, 'learning_rate': 0.00018709248170326016, 'epoch': 6.07}                                                                                             
 18%|█████████████████████▋                                                                                                     | 588/3340 [54:58<4:04:32,  5.33s/it]/home/ubuntu/llm/meta-llama/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.0043, 'learning_rate': 0.0001827012641383899, 'epoch': 7.02}                                                                                              
{'loss': 0.0037, 'learning_rate': 0.00017831004657351964, 'epoch': 7.22}                                                                                             
 20%|████████████████████████▎                                                                                                | 672/3340 [1:02:48<3:57:03,  5.33s/it]/home/ubuntu/llm/meta-llama/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.0036, 'learning_rate': 0.00017391882900864936, 'epoch': 8.16}                                                                                             
 23%|███████████████████████████▍                                                                                             | 756/3340 [1:10:39<3:49:36,  5.33s/it]/home/ubuntu/llm/meta-llama/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.0034, 'learning_rate': 0.00016952761144377912, 'epoch': 9.11}                                                                                             
{'train_runtime': 4710.8131, 'train_samples_per_second': 4.246, 'train_steps_per_second': 0.709, 'train_loss': 0.07250816685458024, 'epoch': 9.25}                   
 25%|██████████████████████████████▍                                                                                          | 840/3340 [1:18:30<3:53:40,  5.61s/it]
> INFO    Finished training, saving model...
(llama) ubuntu@ip-172-31-33-88:~/llm/meta-llama$ 
(llama) ubuntu@ip-172-31-33-88:~/llm/meta-llama$ 

